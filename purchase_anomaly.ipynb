{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pandas 2.2.2\n",
      "Uninstalling pandas-2.2.2:\n",
      "  Successfully uninstalled pandas-2.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: nltk 3.8.1\n",
      "Uninstalling nltk-3.8.1:\n",
      "  Successfully uninstalled nltk-3.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping spacy as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: scipy 1.13.0\n",
      "Uninstalling scipy-1.13.0:\n",
      "  Successfully uninstalled scipy-1.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall pandas --yes\n",
    "!pip uninstall nltk --yes\n",
    "!pip uninstall spacy --yes\n",
    "!pip uninstall scipy --yes\n",
    "!pip uninstall gensim --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.34.84)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.84 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.34.84)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.84->boto3) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.84->boto3) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.84->boto3) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Installing collected packages: pandas\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-data-insights 0.3.3 requires scipy>=1.4.1, which is not installed.\n",
      "statsmodels 0.14.1 requires scipy!=1.9.2,>=1.4, which is not installed.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pandas-2.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting spacy\n",
      "  Using cached spacy-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Using cached thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.7.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached blis-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.17.2)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Using cached spacy-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.2 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (204 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached blis-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "Using cached cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Using cached marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 wasabi-1.1.3 weasel-0.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting scipy\n",
      "  Using cached scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.26.4)\n",
      "Using cached scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
      "Installing collected packages: scipy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scipy-1.15.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gensim\n",
      "  Using cached gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Using cached gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "Using cached scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "Installing collected packages: scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.1\n",
      "    Uninstalling scipy-1.15.1:\n",
      "      Successfully uninstalled scipy-1.15.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gensim-4.3.3 scipy-1.13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "# !pip install snowflake-connector-python\n",
    "!pip install pandas #==2.0.3\n",
    "!pip install nltk #==3.8.1\n",
    "!pip install spacy #==3.5.2\n",
    "!pip install scipy #==1.10.1\n",
    "!pip install gensim #==4.3.1\n",
    "# !pip install snowflake\n",
    "!python -m spacy download en\n",
    "# !pip install --upgrade numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.5\n",
      "  Using cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Using cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.23.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting scipy==1.11.4\n",
      "  Using cached scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy==1.11.4) (1.23.5)\n",
      "Using cached scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.13.1\n",
      "    Uninstalling scipy-1.13.1:\n",
      "      Successfully uninstalled scipy-1.13.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scipy-1.11.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.23.5\n",
    "!pip install scipy==1.11.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f95521d0310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "# import snowflake\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snowflake import connector\n",
    "# from snowflake.snowpark import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /tmp...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import utils\n",
    "import os\n",
    "from io import StringIO\n",
    "import boto3\n",
    "\n",
    "# from snowflake_conn import SnowflakeConnection as sc\n",
    "\n",
    "\n",
    "class main:\n",
    "\n",
    "#     def __init__(self, variables: dict, sdks: dict):\n",
    "    def __init__(self):\n",
    "        # define clients\n",
    "        self.s3 = boto3.client('s3')\n",
    "#         self.s3resource = sdks['s3_resource']\n",
    "#         self.dynamodb = sdks[\"dynamodb_client\"]\n",
    "#         self.db_resource = sdks[\"dynamodb_resource\"]\n",
    "\n",
    "#         # environ variables - dont need to do this but easier to read in below methods.\n",
    "#         self.dynamodbTable = variables['results_table']\n",
    "#         self.db_index = variables['results_table_index']\n",
    "#         self.db_table = self.db_resource.Table(self.dynamodbTable)\n",
    "\n",
    "    \n",
    "    def load_data(self, bucket: str, key: str) -> pd.DataFrame:\n",
    "        ################### read in data from the day ##################\n",
    "        # s3 = boto3.client('s3')\n",
    "        # # POs data\n",
    "        obj = self.s3.get_object(Bucket=bucket, Key=key) \n",
    "        df = pd.read_csv(obj['Body'], low_memory=False)\n",
    "        return df\n",
    "    \n",
    "    def export_data(self, df: pd.DataFrame, bucket: str, key: str) -> None:\n",
    "        ############# writing data to S3 ################\n",
    "        print('Exporting Results to S3')\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "        self.s3.put_object(Body=csv_buffer.getvalue(), Bucket=bucket, Key=key)\n",
    "        print('Completed')\n",
    "    \n",
    "    \n",
    "    def load_snowflake_data(self, date_to_query_on: str) -> pd.DataFrame:\n",
    "        '''\n",
    "        This function will query the snowflake table and return a dataframe.\n",
    "        Parameter:\n",
    "            date_to_query_on (str): date to query on in the format of '%Y-%m-%d %H:%M:%S.%f'\n",
    "        Return:\n",
    "            df_snow (pd.DataFrame): dataframe of snowflake table\n",
    "        '''\n",
    "        # ############## process today's data ##############\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "#         sf_connection = sc(local=True, use_prod=True, aws_secret='snowflake-reporting', aws_region='us-east-2', aws_profile=None, tag=\"Purchasing Anomalies\")\n",
    "#         print(f'Date to query on: {date_to_query_on}')\n",
    "#         # df_snow = sf_connection.query(sql_statement=f'''select * from ZINT_PR.UI_ALL.VW_RPT_PURCHASE_ANOMALIES_MODEL where \"BI Last Updated\" >= '{date_to_query_on}';''', to_df=True)\n",
    "#         df_snow = sf_connection.query(sql_statement=f'''select PAM.*, ZM.\"User Desc\" from ZINT_PR.UI_ALL.VW_RPT_PURCHASE_ANOMALIES_MODEL as PAM\n",
    "#                                 left join (select \"User ID\", \"User Desc\" from \"ZINT_MASTER\".\"UI_SEC\".\"AD Account\") AS ZM on ZM.\"User ID\" = PAM.\"Requisitioner Master\"\n",
    "#                                 where PAM.\"BI Last Updated\" >= '{date_to_query_on}' \n",
    "#                                 and PAM.\"Plant Desc\" in ('Seaford (US) MFG','Camden Polymer (US) MFG','Houston (US) MFG','Longview (US) MFG', 'Orange (US) MFG', 'Victoria (US) MFG');''', # Orange (US) MFG # Victoria (US) MFG # Orange (US) MFG # Camden Polymer (US) MFG\n",
    "#                                 to_df=True)\n",
    "        df_snow = self.load_data(bucket='invista-development-datalake', key='data_science_models/purchasing_anomaly/interim/Purchase Anomaly.csv')\n",
    "#         df_snow = sf_connection.query(sql_statement=f'''select PAM.*, ZM.\"User Desc\" from ZINT_PR.UI_ALL.VW_RPT_PURCHASE_ANOMALIES_MODEL as PAM\n",
    "#                                         left join (select \"User ID\", \"User Desc\" from \"ZINT_MASTER\".\"UI_SEC\".\"AD Account\") AS ZM on ZM.\"User ID\" = PAM.\"Requisitioner Master\"\n",
    "#                                         where PAM.\"BI Last Updated\" >= '{date_to_query_on}' \n",
    "#                                         and PAM.\"Plant Desc\"='Orange (US) MFG';''', # Orange (US) MFG # Victoria (US) MFG # Orange (US) MFG # Camden Polymer (US) MFG\n",
    "#                                         to_df=True)\n",
    "        df_snow.rename(columns={'User Desc': 'requisitioner_name'}, inplace=True)\n",
    "        df_snow.columns = df_snow.columns.str.replace(' ', '_').str.lower()\n",
    "        df_snow['tdname'] = df_snow['purchase_doc_id'].astype(str) + df_snow['purchase_doc_item_id'].astype(str)\n",
    "        df_snow['tdname'] = df_snow['tdname'].astype(np.int64)\n",
    "#         df_snow['bi_last_updated'] = pd.to_datetime(df_snow['bi_last_updated'])\n",
    "\n",
    "        return df_snow\n",
    "    \n",
    "\n",
    "    def preprocessing_data(self):\n",
    "        '''This function will load the data from S3 and Snowflake, merge them, clean text data and export the data back to S3.'''\n",
    "\n",
    "#         key = \"data_science_models/purchasing_anomaly/interim/purchase_anomaly_masterfile.csv\"\n",
    "#         df = self.load_data(bucket=\"invista-development-datalake\", key=key)\n",
    "#         df['tdname'] = df['tdname'].astype(np.int64)\n",
    "#         df['bi_last_updated'] = pd.to_datetime(df['bi_last_updated'], format='mixed')\n",
    "\n",
    "#         date_to_query_on = datetime.strftime(df['bi_last_updated'].max(), '%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "        date_to_query_on = '2023-01-01 00:00:00.000'\n",
    "# # #         date_to_query_on = datetime.strftime(pd.to_datetime('now'), '%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "        df_snow = self.load_snowflake_data(date_to_query_on=date_to_query_on)\n",
    "\n",
    "        df_snow = utils.get_short_text_by_vendor(df_snow) # adds column \"text_clean\"\n",
    "        df_snow['text_clean'] = utils.clean_text(df_snow['text_clean'])\n",
    "        print('Extracting Noun Words')\n",
    "#         df_snow['noun_words'] = utils.extract_noun_words(df_snow['text_clean'])\n",
    "        df_snow['noun_words'] = utils.text_parallel_clean(df_snow['text_clean'])\n",
    "\n",
    "        df_snow['noun_words'] = df_snow['noun_words'].apply(lambda x: x+'blank' if len(x)==0 else x)\n",
    "        df_snow = utils.get_noun_words_reduced(df=df_snow)\n",
    "        \n",
    "        # types\n",
    "        df_snow['tdname'] = df_snow['tdname'].astype(np.int64)\n",
    "        df_snow['bi_last_updated'] = pd.to_datetime(df_snow['bi_last_updated'].str.replace('Z','')[:-2])\n",
    "        \n",
    "#         df_cols = list(df.columns)\n",
    "#         dff = pd.concat([df[df_cols], df_snow[df_cols]], axis=0)\n",
    "#         dff = dff.sort_values(by=['bi_last_updated'], ascending=True).drop_duplicates(subset=['tdname'], keep='last').copy()\n",
    "        \n",
    "        return df_snow\n",
    "        \n",
    "#         key = 'data_science_models/purchasing_anomaly/interim/purchase_anomaly_masterfile.csv'\n",
    "#         self.export_data(df=dff, bucket=\"invista-development-datalake\", key=key)\n",
    "        \n",
    "#         # trigger analysis\n",
    "#         key = 'data_science_models/purchasing_anomaly/results/purchase_anomaly_masterfile.csv'\n",
    "#         self.export_data(df=dff, bucket=\"invista-development-datalake\", key=key)\n",
    "        print('--- Data has been Exported ---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Noun Words\n"
     ]
    }
   ],
   "source": [
    "dff = main().preprocessing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2024-06-06 23:06:25.553\n",
       "1   2024-10-28 23:04:15.472\n",
       "2   2024-11-03 22:04:14.305\n",
       "3   2024-10-28 23:04:15.472\n",
       "4   2024-04-14 23:07:54.246\n",
       "Name: bi_last_updated, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff['bi_last_updated'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff.to_csv('orange_purchase_anomaly_masterfile.csv', index=False)\n",
    "dff.to_csv('purchase_anomaly_masterfile.csv', index=False)\n",
    "# dff.to_csv('camden_purchase_anomaly_masterfile.csv', index=False)\n",
    "# dff = pd.read_csv('camden_purchase_anomaly_masterfile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff = pd.read_csv('purchase_anomaly_masterfile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff.sort_values(by=['purchase_doc_date'], ascending=False)\n",
    "dff['purchase_doc_date'] = pd.to_datetime(dff['purchase_doc_date'])\n",
    "# min_date = dff['purchase_doc_date'].max() - pd.Timedelta(days=450)#(days=365+182)\n",
    "# dff_year = dff[dff['purchase_doc_date']>=min_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = dff['noun_words_reduced'].str.split()\n",
    "# for w in a:\n",
    "#     print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_categories(df: pd.DataFrame, use_annotations: bool=False, df_labels=None) -> pd.DataFrame:\n",
    "    \"\"\"Reduced noun_words will be split into common_words and rare_words.\n",
    "    A unique word count is given to each record.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe with noun_words_reduced column\n",
    "        use_annotations (bool, optional): When use_annotations=True, then if red_flag = 1,\n",
    "            then the words in the record are not considered in the corpus.\n",
    "            Defaults to False.\n",
    "        df_labels (pd.DataFrame, optional): df_labels data in s3. Defaults to None to pull labels from DynamoDB table.\n",
    "    \"\"\"\n",
    "#     start_date = df['purchase_doc_date'].max() - pd.Timedelta(days=450)\n",
    "#     df = df[df['purchase_doc_date']>start_date].copy()\n",
    "#     df = df.merge(df_labels, on='tdname', how='left')\n",
    "\n",
    "    df['noun_words_reduced'] = df['noun_words_reduced'].replace(np.nan, 'blank')\n",
    "    df['noun_words_reduced'] = df['noun_words_reduced'].str.split()\n",
    "\n",
    "    dff = pd.DataFrame()\n",
    "    for plant in df['plant_desc'].unique():\n",
    "        print(f'Processing {plant}')\n",
    "\n",
    "        df_plant = df[df['plant_desc']==plant].copy()\n",
    "\n",
    "        all_text = []\n",
    "        if use_annotations:\n",
    "            for i in df_plant[df_plant['annotation']!=1]['noun_words_reduced']:\n",
    "                all_text.extend(i)\n",
    "        else:\n",
    "            for i in df_plant['noun_words_reduced']:\n",
    "                all_text.extend(i)\n",
    "            \n",
    "        ###################\n",
    "        text_series = pd.Series(all_text)\n",
    "        rare_words = list(text_series.value_counts()[text_series.value_counts()<=3].index)\n",
    "        rare_words_list = []\n",
    "        common_words_list = []\n",
    "        unique_count = []\n",
    "        for ind in df_plant['noun_words_reduced'].index:\n",
    "            unique_counter = 0\n",
    "            rare_words_found = []\n",
    "            common_words_found = []\n",
    "            for w in df_plant.loc[ind, 'noun_words_reduced']:\n",
    "                if w in rare_words:\n",
    "                    unique_counter+=1\n",
    "                    rare_words_found.append(w)\n",
    "                else:\n",
    "                    common_words_found.append(w)\n",
    "            rw = ' '.join(rare_words_found)\n",
    "            cw = ' '.join(common_words_found)\n",
    "            rare_words_list.append(rw)\n",
    "            common_words_list.append(cw)\n",
    "            unique_count.append(unique_counter)\n",
    "            \n",
    "        df_plant['unique_count'] = unique_count\n",
    "        df_plant['common_words'] = common_words_list\n",
    "        df_plant['rare_words'] = rare_words_list\n",
    "        df_plant['noun_words_reduced'] = df_plant['noun_words_reduced'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "        dff = pd.concat([dff, df_plant], axis=0)\n",
    "    return dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- getting word categories for Houston (US) MFG -----\n",
      "Plant:Houston (US) MFG, Shape:(2405, 41)\n",
      "Processing Houston (US) MFG\n",
      "----- getting word categories for Kingston (CA) MFG -----\n",
      "Plant:Kingston (CA) MFG, Shape:(21414, 41)\n",
      "Processing Kingston (CA) MFG\n",
      "----- getting word categories for Longview (US) MFG -----\n",
      "Plant:Longview (US) MFG, Shape:(1860, 41)\n",
      "Processing Longview (US) MFG\n",
      "----- getting word categories for Camden Polymer (US) MFG -----\n",
      "Plant:Camden Polymer (US) MFG, Shape:(9081, 41)\n",
      "Processing Camden Polymer (US) MFG\n",
      "----- getting word categories for Victoria (US) MFG -----\n",
      "Plant:Victoria (US) MFG, Shape:(14401, 41)\n",
      "Processing Victoria (US) MFG\n",
      "----- getting word categories for Seaford (US) MFG -----\n",
      "Plant:Seaford (US) MFG, Shape:(3581, 41)\n",
      "Processing Seaford (US) MFG\n",
      "----- getting word categories for Orange (US) MFG -----\n",
      "Plant:Orange (US) MFG, Shape:(2441, 41)\n",
      "Processing Orange (US) MFG\n",
      "----- getting word categories for Maitland (CA)  MFG -----\n",
      "Plant:Maitland (CA)  MFG, Shape:(5510, 41)\n",
      "Processing Maitland (CA)  MFG\n"
     ]
    }
   ],
   "source": [
    "# df_master = get_word_categories(dff_year).copy()\n",
    "\n",
    "def get_word_categories_by_plant(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "    plant_list = list(set(df['plant_desc']))\n",
    "    dff = pd.DataFrame()\n",
    "\n",
    "    for plant in plant_list:\n",
    "        print(f'----- getting word categories for {plant} -----')\n",
    "        df_plant = df[df['plant_desc']==plant].copy()\n",
    "        print(f'Plant:{plant}, Shape:{df_plant.shape}')\n",
    "        df_plant = get_word_categories(df_plant)\n",
    "        dff = pd.concat([dff, df_plant], axis=0)\n",
    "\n",
    "    return dff\n",
    "\n",
    "df_master = get_word_categories_by_plant(df=dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_desc</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Camden Polymer (US) MFG</th>\n",
       "      <td>9081.0</td>\n",
       "      <td>0.431781</td>\n",
       "      <td>0.773458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Houston (US) MFG</th>\n",
       "      <td>2405.0</td>\n",
       "      <td>0.920998</td>\n",
       "      <td>1.126557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kingston (CA) MFG</th>\n",
       "      <td>21414.0</td>\n",
       "      <td>0.189315</td>\n",
       "      <td>0.546554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longview (US) MFG</th>\n",
       "      <td>1860.0</td>\n",
       "      <td>0.765054</td>\n",
       "      <td>1.050200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maitland (CA)  MFG</th>\n",
       "      <td>5510.0</td>\n",
       "      <td>0.330672</td>\n",
       "      <td>0.811686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orange (US) MFG</th>\n",
       "      <td>2441.0</td>\n",
       "      <td>0.708726</td>\n",
       "      <td>1.084145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seaford (US) MFG</th>\n",
       "      <td>3581.0</td>\n",
       "      <td>0.707344</td>\n",
       "      <td>1.018721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Victoria (US) MFG</th>\n",
       "      <td>14401.0</td>\n",
       "      <td>0.297896</td>\n",
       "      <td>0.668956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           count      mean       std  min  25%  50%  75%  max\n",
       "plant_desc                                                                   \n",
       "Camden Polymer (US) MFG   9081.0  0.431781  0.773458  0.0  0.0  0.0  1.0  5.0\n",
       "Houston (US) MFG          2405.0  0.920998  1.126557  0.0  0.0  0.0  2.0  5.0\n",
       "Kingston (CA) MFG        21414.0  0.189315  0.546554  0.0  0.0  0.0  0.0  5.0\n",
       "Longview (US) MFG         1860.0  0.765054  1.050200  0.0  0.0  0.0  1.0  5.0\n",
       "Maitland (CA)  MFG        5510.0  0.330672  0.811686  0.0  0.0  0.0  0.0  5.0\n",
       "Orange (US) MFG           2441.0  0.708726  1.084145  0.0  0.0  0.0  1.0  5.0\n",
       "Seaford (US) MFG          3581.0  0.707344  1.018721  0.0  0.0  0.0  1.0  5.0\n",
       "Victoria (US) MFG        14401.0  0.297896  0.668956  0.0  0.0  0.0  0.0  5.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master.groupby('plant_desc')['unique_count'].describe() #.agg(['mean','median','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_master[df_master['unique_count']>=2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Maitland (CA)  MFG',\n",
       " 'Houston (US) MFG',\n",
       " 'Seaford (US) MFG',\n",
       " 'Orange (US) MFG',\n",
       " 'Kingston (CA) MFG',\n",
       " 'Victoria (US) MFG',\n",
       " 'Camden Polymer (US) MFG',\n",
       " 'Longview (US) MFG']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(df['plant_desc']))\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_old.loc[:,'uid'] = df_old['purchase_doc_id'].astype(str) + df_old['purchase_doc_item_id'].astype(str)\n",
    "df.loc[:,'uid'] = df['purchase_doc_id'].astype(str) + df['purchase_doc_item_id'].astype(str)\n",
    "\n",
    "max_date = df['purchase_doc_date'].max()\n",
    "new_date = max_date - pd.Timedelta(days=95)\n",
    "\n",
    "df = df.loc[(df['purchase_doc_date']>= new_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Add logic to remove previous file's purchase's\n",
    "prior_plant_flagged_purchases = {\n",
    "    'Victoria (US) MFG':[],      \n",
    "    'Seaford (US) MFG':[],\n",
    "    'Longview (US) MFG':[],\n",
    "    'Camden Polymer (US) MFG':[],\n",
    "    'Orange (US) MFG':[],\n",
    "    'Maitland (CA)  MFG':[],\n",
    "    'Kingston (CA) MFG':[],\n",
    "    'Houston (US) MFG':[]\n",
    "}\n",
    "\n",
    "for plant_name in prior_plant_flagged_purchases:\n",
    "    df1 = pd.read_excel('purchase_anomaly_09232024.xlsx', sheet_name=plant_name)\n",
    "    df2 = pd.read_excel('purchase_anomaly_09302024.xlsx', sheet_name=plant_name)\n",
    "    df3 = pd.read_excel('purchase_anomaly_10072024.xlsx', sheet_name=plant_name)\n",
    "    df4 = pd.read_excel('purchase_anomaly_10142024.xlsx', sheet_name=plant_name)\n",
    "    df5 = pd.read_excel('purchase_anomaly_10212024.xlsx', sheet_name=plant_name)\n",
    "    df6 = pd.read_excel('purchase_anomaly_10282024.xlsx', sheet_name=plant_name)\n",
    "    df7 = pd.read_excel('purchase_anomaly_11042024.xlsx', sheet_name=plant_name)\n",
    "    df8 = pd.read_excel('purchase_anomaly_11112024.xlsx', sheet_name=plant_name)\n",
    "    df9 = pd.read_excel('purchase_anomaly_11182024.xlsx', sheet_name=plant_name)\n",
    "    df10 = pd.read_excel('purchase_anomaly_11252024.xlsx', sheet_name=plant_name)\n",
    "    df11 = pd.read_excel('purchase_anomaly_12022024.xlsx', sheet_name=plant_name)\n",
    "    df12 = pd.read_excel('purchase_anomaly_12092024.xlsx', sheet_name=plant_name)\n",
    "    df13 = pd.read_excel('purchase_anomaly_12162024.xlsx', sheet_name=plant_name)\n",
    "    df14 = pd.read_excel('purchase_anomaly_12232024.xlsx', sheet_name=plant_name)\n",
    "    df15 = pd.read_excel('purchase_anomaly_12302024.xlsx', sheet_name=plant_name)\n",
    "    df16 = pd.read_excel('purchase_anomaly_01062025.xlsx', sheet_name=plant_name)\n",
    "    df17 = pd.read_excel('purchase_anomaly_01132025.xlsx', sheet_name=plant_name)\n",
    "    df18 = pd.read_excel('purchase_anomaly_01202025.xlsx', sheet_name=plant_name)\n",
    "    \n",
    "    \n",
    "    cols_to_use = df1.columns\n",
    "    df_old = pd.concat([\n",
    "        df1, df2[cols_to_use], df3[cols_to_use], df4[cols_to_use], df5[cols_to_use], df6[cols_to_use], df7[cols_to_use],\n",
    "        df8[cols_to_use], df9[cols_to_use], df10[cols_to_use], df11[cols_to_use], df12[cols_to_use], df13[cols_to_use],\n",
    "        df14[cols_to_use], df15[cols_to_use], df16[cols_to_use], df17[cols_to_use], df18[cols_to_use]\n",
    "    ], axis=0)\n",
    "    df_old.loc[:,'uid'] = df_old['purchase_doc_id'].astype(str) + df_old['purchase_doc_item_id'].astype(str)\n",
    "    prior_plant_flagged_purchases[plant_name].extend(df_old['uid'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_doc_date</th>\n",
       "      <th>purchase_doc_month</th>\n",
       "      <th>purchase_doc_quarter</th>\n",
       "      <th>purchase_doc_year</th>\n",
       "      <th>requested_delivery_date</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>vendor_nm</th>\n",
       "      <th>plant_desc</th>\n",
       "      <th>purchase_doc_id</th>\n",
       "      <th>purchase_doc_item_id</th>\n",
       "      <th>...</th>\n",
       "      <th>po_item_net_order_value_usd</th>\n",
       "      <th>requisitioner_name</th>\n",
       "      <th>tdname</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>noun_words</th>\n",
       "      <th>noun_words_reduced</th>\n",
       "      <th>unique_count</th>\n",
       "      <th>common_words</th>\n",
       "      <th>rare_words</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-19 00:00:00</td>\n",
       "      <td>2023-09</td>\n",
       "      <td>2023 QTR-3</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-09-21</td>\n",
       "      <td>8010008932</td>\n",
       "      <td>STAPLES ADVANTAGE</td>\n",
       "      <td>Houston (US) MFG</td>\n",
       "      <td>2000085217</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>17.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200008521750</td>\n",
       "      <td>seattle best coffee post alley blend</td>\n",
       "      <td>blend post coffee alley seattle</td>\n",
       "      <td>seattle coffee post alley blend</td>\n",
       "      <td>2</td>\n",
       "      <td>coffee post blend</td>\n",
       "      <td>seattle alley</td>\n",
       "      <td>200008521750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-07-07 00:00:00</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>2024 QTR-3</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-07-12</td>\n",
       "      <td>8010099066</td>\n",
       "      <td>AMAZON.COM SERVICES INC</td>\n",
       "      <td>Houston (US) MFG</td>\n",
       "      <td>2000114892</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>989.97</td>\n",
       "      <td>Chauvette, Scott</td>\n",
       "      <td>200011489220</td>\n",
       "      <td>btu portable air conditioner cool</td>\n",
       "      <td>portable btu air cool conditioner</td>\n",
       "      <td>btu portable air conditioner cool</td>\n",
       "      <td>4</td>\n",
       "      <td>air</td>\n",
       "      <td>btu portable conditioner cool</td>\n",
       "      <td>200011489220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-06-19 00:00:00</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>2024 QTR-2</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-06-26</td>\n",
       "      <td>8010099066</td>\n",
       "      <td>AMAZON.COM SERVICES INC</td>\n",
       "      <td>Houston (US) MFG</td>\n",
       "      <td>2000113284</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>13.39</td>\n",
       "      <td>Thornton, Erin</td>\n",
       "      <td>200011328420</td>\n",
       "      <td>oregon farm fresh snacks wasabi pea mix</td>\n",
       "      <td>wasabi snacks oregon mix farm pea</td>\n",
       "      <td>oregon farm snacks wasabi pea</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oregon farm snacks wasabi pea</td>\n",
       "      <td>200011328420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-05-02 00:00:00</td>\n",
       "      <td>2024-05</td>\n",
       "      <td>2024 QTR-2</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-05-06</td>\n",
       "      <td>8010008932</td>\n",
       "      <td>STAPLES ADVANTAGE</td>\n",
       "      <td>Houston (US) MFG</td>\n",
       "      <td>2000108197</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>11.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000108197120</td>\n",
       "      <td>prismacolor premier col erase colored</td>\n",
       "      <td>premier erase colored prismacolor col</td>\n",
       "      <td>prismacolor premier col erase colored</td>\n",
       "      <td>3</td>\n",
       "      <td>col erase</td>\n",
       "      <td>prismacolor premier colored</td>\n",
       "      <td>2000108197120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-04 00:00:00</td>\n",
       "      <td>2023-10</td>\n",
       "      <td>2023 QTR-4</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-10-11</td>\n",
       "      <td>8010008932</td>\n",
       "      <td>STAPLES ADVANTAGE</td>\n",
       "      <td>Houston (US) MFG</td>\n",
       "      <td>2000086855</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>68.35</td>\n",
       "      <td>Williams, Susan</td>\n",
       "      <td>200008685520</td>\n",
       "      <td>expo caddy kit assorted colors</td>\n",
       "      <td>caddy colors expo kit</td>\n",
       "      <td>expo caddy kit colors</td>\n",
       "      <td>2</td>\n",
       "      <td>expo kit</td>\n",
       "      <td>caddy colors</td>\n",
       "      <td>200008685520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     purchase_doc_date purchase_doc_month purchase_doc_quarter  \\\n",
       "0  2023-09-19 00:00:00            2023-09           2023 QTR-3   \n",
       "1  2024-07-07 00:00:00            2024-07           2024 QTR-3   \n",
       "2  2024-06-19 00:00:00            2024-06           2024 QTR-2   \n",
       "3  2024-05-02 00:00:00            2024-05           2024 QTR-2   \n",
       "4  2023-10-04 00:00:00            2023-10           2023 QTR-4   \n",
       "\n",
       "   purchase_doc_year requested_delivery_date   vendor_id  \\\n",
       "0               2023              2023-09-21  8010008932   \n",
       "1               2024              2024-07-12  8010099066   \n",
       "2               2024              2024-06-26  8010099066   \n",
       "3               2024              2024-05-06  8010008932   \n",
       "4               2023              2023-10-11  8010008932   \n",
       "\n",
       "                 vendor_nm        plant_desc  purchase_doc_id  \\\n",
       "0        STAPLES ADVANTAGE  Houston (US) MFG       2000085217   \n",
       "1  AMAZON.COM SERVICES INC  Houston (US) MFG       2000114892   \n",
       "2  AMAZON.COM SERVICES INC  Houston (US) MFG       2000113284   \n",
       "3        STAPLES ADVANTAGE  Houston (US) MFG       2000108197   \n",
       "4        STAPLES ADVANTAGE  Houston (US) MFG       2000086855   \n",
       "\n",
       "   purchase_doc_item_id  ... po_item_net_order_value_usd requisitioner_name  \\\n",
       "0                    50  ...                       17.09                NaN   \n",
       "1                    20  ...                      989.97   Chauvette, Scott   \n",
       "2                    20  ...                       13.39     Thornton, Erin   \n",
       "3                   120  ...                       11.00                NaN   \n",
       "4                    20  ...                       68.35    Williams, Susan   \n",
       "\n",
       "          tdname                               text_clean  \\\n",
       "0   200008521750     seattle best coffee post alley blend   \n",
       "1   200011489220        btu portable air conditioner cool   \n",
       "2   200011328420  oregon farm fresh snacks wasabi pea mix   \n",
       "3  2000108197120    prismacolor premier col erase colored   \n",
       "4   200008685520           expo caddy kit assorted colors   \n",
       "\n",
       "                              noun_words  \\\n",
       "0        blend post coffee alley seattle   \n",
       "1      portable btu air cool conditioner   \n",
       "2      wasabi snacks oregon mix farm pea   \n",
       "3  premier erase colored prismacolor col   \n",
       "4                  caddy colors expo kit   \n",
       "\n",
       "                      noun_words_reduced unique_count       common_words  \\\n",
       "0        seattle coffee post alley blend            2  coffee post blend   \n",
       "1      btu portable air conditioner cool            4                air   \n",
       "2          oregon farm snacks wasabi pea            5                NaN   \n",
       "3  prismacolor premier col erase colored            3          col erase   \n",
       "4                  expo caddy kit colors            2           expo kit   \n",
       "\n",
       "                      rare_words            uid  \n",
       "0                  seattle alley   200008521750  \n",
       "1  btu portable conditioner cool   200011489220  \n",
       "2  oregon farm snacks wasabi pea   200011328420  \n",
       "3    prismacolor premier colored  2000108197120  \n",
       "4                   caddy colors   200008685520  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_old.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Houston (US) MFG\n",
      "(173, 45)\n",
      "(4, 45)\n",
      "Kingston (CA) MFG\n",
      "(178, 45)\n",
      "(21, 45)\n",
      "Longview (US) MFG\n",
      "(128, 45)\n",
      "(15, 45)\n",
      "Camden Polymer (US) MFG\n",
      "(261, 45)\n",
      "(19, 45)\n",
      "Victoria (US) MFG\n",
      "(258, 45)\n",
      "(17, 45)\n",
      "Seaford (US) MFG\n",
      "(148, 45)\n",
      "(19, 45)\n",
      "Orange (US) MFG\n",
      "(103, 45)\n",
      "(6, 45)\n",
      "Maitland (CA)  MFG\n",
      "(124, 45)\n",
      "(11, 45)\n"
     ]
    }
   ],
   "source": [
    "plants = set(list(df['plant_desc']))\n",
    "for plant in plants:\n",
    "    print(plant)\n",
    "    df_plant = df[df['plant_desc']==plant].copy()\n",
    "    print(df_plant.shape)\n",
    "    df_plant = df_plant.loc[(~df_plant['uid'].isin(prior_plant_flagged_purchases[plant]))]\n",
    "    print(df_plant.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a excel writer object\n",
    "with pd.ExcelWriter(\"purchase_anomaly_01272025.xlsx\") as writer:\n",
    "    plants = set(list(df['plant_desc']))\n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    for plant in plants:\n",
    "        df_plant = df[df['plant_desc']==plant].copy()\n",
    "        df_plant = df_plant.loc[(~df_plant['uid'].isin(prior_plant_flagged_purchases[plant]))]\n",
    "        df_plant.to_excel(writer, sheet_name=f\"{plant}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master[df_master['unique_count']>=2]['purchase_document_item_short_text'].head()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
